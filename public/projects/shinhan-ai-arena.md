# 신한은행 - AI Arena | AI 모델 비교 플랫폼

## 프로젝트 개요

- **프로젝트명**: 신한은행 - AI Arena
- **한 줄 소개**: 여러 LLM 모델의 응답을 실시간으로 비교할 수 있는 AI 모델 평가 플랫폼
- **개발 기간**: 2024.07 - 2024.08 (약 2개월)
- **팀 구성**: Frontend 1명, Backend 1명
- **배포 URL**: 사내 전용 서비스
- **기업**: 신한은행

---

## 프로젝트 설명

### 배경 및 목적

신한은행에서 내부 데이터로 여러 LLM을 제작하게 되면서 다양한 LLM(ChatGPT, gemini 등 포함) 모델 중 어떤 것이 특정 업무에 적합한지 판단하는 것이 중요해졌습니다.
**AI Arena**는 여러 AI 모델의 응답을 동시에 비교하고 평가할 수 있는 플랫폼으로, 사용자가 실제 업무 상황에서 최적의 AI 모델을 선택할 수 있도록 돕습니다.
실시간 스트리밍 응답을 통해 즉각적인 피드백을 제공하며, 기존 LLM 서비스와 동일한 UX로 학습 없이 바로 사용할 수 있습니다.

### 주요 기능

- **실시간 스트리밍 응답**: SSE를 활용한 실시간 AI 응답 스트리밍으로 대기 시간 최소화
- **멀티 모델 비교**: 여러 LLM 모델의 응답을 동시에 확인하고 비교 가능
- **직관적인 UX**: 기존 LLM 서비스와 동일한 인터페이스로 별도 학습 없이 즉시 사용 가능

---

## 담당 역할 및 기여도

- **역할**: Frontend 개발
- **기여도**: 전체 프론트엔드 개발 담당
- **담당 파트**:
  - SSE 기반 실시간 스트리밍 응답 시스템 구현
  - 멀티 모델 병렬 요청 및 응답 처리 아키텍처 설계
  - LLM UI/UX 설계 및 구현
  - 스트리밍 예외 처리 및 안정성 확보

---

## 기술 스택

**Frontend**

- Next.js, TypeScript
- recoil (전역 상태 관리)
- Tanstack Query, axios (서버 상태 관리)
- module.scss (스타일링)
- Server-Sent Events (SSE) (실시간 스트리밍)

**Build & Deploy**

- 사내 배포 환경

**Collaboration**

- 내부망 GitLab (코드 버전관리)
- Figma (UI 설계 협업)

---

## 주요 구현 내용

### 1. SSE 기반 실시간 스트리밍 응답 시스템

- 사용자가 질문 후 전체 응답을 기다리지 않고 즉시 응답을 확인할 수 있도록 구현
- 기존 ChatGPT와 같은 LLM 서비스와 동일한 타이핑 효과로 자연스러운 UX 제공

**구현 내용**

**1) Server-Sent Events(SSE)를 활용한 실시간 스트리밍**

> 사용자가 AI 응답을 기다리는 시간을 최소화하고, 응답이 생성되는 과정을 실시간으로 확인할 수 있도록 SSE를 도입했습니다.

- **실시간 응답 출력**
  서버에서 생성되는 토큰을 즉시 받아 UI에 점진적으로 표시하여 체감 대기시간 대폭 감소

- **자연스러운 타이핑 효과**
  토큰 단위로 텍스트가 추가되며 마치 사람이 타이핑하는 듯한 자연스러운 UX 구현

**2) 안정적인 스트리밍 연결 관리**

> 사용자의 다양한 행동 패턴(탭 종료, 페이지 이탈 등)에 대응하여 런타임 오류 없이 안정적인 서비스를 제공하기 위한 처리 로직을 구현했습니다.

- **AbortController 기반 연결 제어**
  컴포넌트 언마운트, 페이지 이탈 시 진행 중인 스트리밍을 안전하게 종료

- **EventSource.close() 활용**
  스트리밍 완료 또는 중단 시 명시적으로 연결을 종료하여 메모리 누수 방지

- **에러 핸들링**
  네트워크 오류, 타임아웃 등 예외 상황에 대한 적절한 사용자 피드백 제공

### 2. 멀티 모델 병렬 요청 시스템

- 여러 AI 모델의 응답을 동시에 확인하고 비교할 수 있는 기능 제공
- 순차 처리 대비 획기적인 응답 시간 단축

**구현 내용**

**1) Promise.All을 활용한 병렬 처리**

> 여러 AI 모델에 순차적으로 요청하면 불필요한 대기 시간이 발생하여, 병렬 요청으로 모든 모델의 응답을 동시에 받아 즉시 비교할 수 있도록 구현했습니다.

- **동시 다발적 스트리밍**
  여러 AI 모델에 동시에 요청을 보내고 각각의 응답을 독립적으로 스트리밍

- **즉시 비교 가능한 UX**
  사용자가 각 모델의 응답 속도, 품질, 특성을 실시간으로 비교 가능

**2) 독립적인 스트림 관리**

> 각 AI 모델의 응답이 서로 영향을 주지 않도록 독립적으로 관리하여 안정성을 확보했습니다.

- **개별 상태 관리**
  각 모델별로 로딩, 스트리밍, 완료, 에러 상태를 독립적으로 관리

- **부분 실패 처리**
  일부 모델의 요청이 실패해도 다른 모델의 응답은 정상적으로 표시

### 3. 직관적인 LLM UI/UX 설계

- 사용자가 별도의 학습 없이 바로 사용할 수 있도록 기존 LLM 서비스와 동일한 인터페이스 제공

**구현 내용**

**1) ChatGPT 스타일 UI 구현**

> 일부 사용자들이 기존 LLM 서비스로 착각할 정도로 유사한 UX를 제공하여 학습 곡선을 최소화했습니다.

- **친숙한 채팅 인터페이스**
  메시지 입력창, 대화 히스토리, 타이핑 효과 등 익숙한 UI 요소 활용

- **직관적인 비교 레이아웃**
  여러 모델의 응답을 나란히 배치하여 한눈에 비교 가능

**2) 시각적 피드백 제공**

> 스트리밍 상태, 응답 완료 여부 등을 명확하게 전달하여 사용자가 현재 상태를 쉽게 파악할 수 있도록 했습니다.

- **로딩 인디케이터**
  각 모델별 응답 대기 중임을 표시

- **완료 표시**
  스트리밍이 완료되었음을 시각적으로 안내

---

## 트러블슈팅 & 문제 해결

### 문제 1: 스트리밍 중단 시 메모리 누수 및 런타임 오류

**상황**

- 사용자가 스트리밍 중 페이지를 이탈하거나 탭을 닫을 때 EventSource 연결이 정리되지 않아 메모리 누수 발생
- 컴포넌트가 언마운트된 후에도 스트리밍 이벤트가 계속 발생하여 런타임 오류 발생

**해결 과정**

1. **AbortController 도입**
   컴포넌트 언마운트 시 useEffect cleanup에서 AbortController.abort()를 호출하여 진행 중인 요청 취소

2. **EventSource 명시적 종료**
   스트리밍 완료, 에러 발생, 사용자 중단 시 EventSource.close()를 호출하여 연결 확실히 종료

3. **상태 업데이트 가드 추가**
   컴포넌트 언마운트 여부를 체크하여 언마운트된 컴포넌트에 상태 업데이트 시도하지 않도록 방어 로직 추가

**결과**

- 메모리 누수 문제 완전 해결
- 스트리밍 중 페이지 이탈 시에도 런타임 오류 0% 달성
- 운영 안정성 대폭 향상

### 문제 2: 병렬 스트리밍 시 UI 렌더링 성능 저하

**상황**

- 여러 모델의 응답이 동시에 스트리밍되면서 초당 수십 번의 상태 업데이트 발생
- 잦은 리렌더링으로 UI가 버벅거리고 타이핑 효과가 부자연스러워짐

**해결 과정**

1. **React.memo 최적화**
   각 모델의 응답 컴포넌트를 React.memo로 감싸 불필요한 리렌더링 방지

2. **가상화 적용**
   대화 히스토리가 길어질 경우 가시 영역만 렌더링하는 가상 스크롤 적용

**결과**

- 자연스러운 타이핑 효과 구현
- 사용자 경험 대폭 개선

---

## 회고

신한은행 AI Arena 프로젝트는 현업 사용자들에게 즉각적으로 피드백을 받으며 특히 예외처리나 에러핸들링에 대해 많은 성장을 이룬 프로젝트였습니다.

SSE를 활용한 실시간 스트리밍 구현은 처음 다뤄보는 기술이었지만, 사용자 경험을 획기적으로 개선할 수 있는 강력한 도구임을 깨달았습니다. 또한 병렬 처리를 통해 성능을 최적화하고, 다양한 예외 상황을 안정적으로 처리하는 방법을 배웠습니다.

기존 LLM 서비스와 동일한 수준의 UX를 구현하면서, "좋은 개발자는 기술만 잘하는 것이 아니라 사용자가 편하게 쓸 수 있게 만드는 것"이라는 것을 다시 한번 느꼈습니다.

실제로 일부 사용자들이 기존 LLM 서비스로 착각할 정도로 자연스러운 UX를 제공할 수 있었던 점이 가장 뿌듯했습니다. ( 어드민 페이지에서 사용자들이 어떤 질문을 했는지 물어볼 수 있는데 은행 업무 관련 질문이 아니라 뜬금없이 코드가 있는 경우가 있습니다. ㅎㅎ )

다만, 더 다양한 AI 모델과 평가 지표를 추가하지 못한 점은 아쉬움으로 남습니다.

추후 점수 계산을 하는 프로그램을 만들 기회가 생긴다면 된다면 더 치밀한 레이팅 설계를 해 볼 예정입니다.
